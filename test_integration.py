"""
Integration tests for the DrumToMIDI pipeline.

Tests the complete workflow: separate → cleanup → midi → video
to ensure refactoring doesn't break functionality.

These tests use synthetic audio/stems to run quickly without ML models.
"""

import pytest
import numpy as np
import soundfile as sf
import shutil
import json
from pathlib import Path
from typing import Generator, Dict, Any

# ============================================================================
# Test Fixtures
# ============================================================================

@pytest.fixture
def temp_user_files(tmp_path: Path) -> Generator[Path, None, None]:
    """Create a temporary user_files directory structure."""
    user_files = tmp_path / "user_files"
    user_files.mkdir()
    yield user_files
    # Cleanup handled by tmp_path fixture


@pytest.fixture
def sample_rate() -> int:
    """Standard sample rate for tests."""
    return 44100


@pytest.fixture
def synthetic_audio(sample_rate: int) -> np.ndarray:
    """
    Generate synthetic drum audio (1 second).
    
    Creates a simple audio with:
    - Kick hits at 0.0s and 0.5s (low frequency burst)
    - Snare hit at 0.25s and 0.75s (mid-high frequency burst)
    """
    duration = 1.0
    num_samples = int(duration * sample_rate)
    audio = np.zeros((num_samples, 2), dtype=np.float32)
    
    def add_hit(audio: np.ndarray, time: float, freq: float, decay: float = 0.1):
        """Add a drum hit at the specified time."""
        start = int(time * sample_rate)
        hit_samples = int(decay * sample_rate)
        t = np.linspace(0, decay, hit_samples)
        envelope = np.exp(-t * 20)
        hit = np.sin(2 * np.pi * freq * t) * envelope
        end = min(start + hit_samples, len(audio))
        audio[start:end, 0] += hit[:end-start]
        audio[start:end, 1] += hit[:end-start]
    
    # Add kick hits (low frequency)
    add_hit(audio, 0.0, 60)
    add_hit(audio, 0.5, 60)
    
    # Add snare hits (higher frequency)
    add_hit(audio, 0.25, 200)
    add_hit(audio, 0.75, 200)
    
    # Normalize
    audio = audio / np.max(np.abs(audio)) * 0.8
    return audio


@pytest.fixture
def synthetic_stems(sample_rate: int) -> Dict[str, np.ndarray]:
    """
    Generate synthetic separated stems.
    
    Creates minimal stems that mimic the output of separation.
    """
    duration = 1.0
    num_samples = int(duration * sample_rate)
    
    def create_stem(hit_times: list, freq: float, decay: float = 0.1) -> np.ndarray:
        """Create a stem with hits at specified times."""
        audio = np.zeros((num_samples, 2), dtype=np.float32)
        for time in hit_times:
            start = int(time * sample_rate)
            hit_samples = int(decay * sample_rate)
            t = np.linspace(0, decay, hit_samples)
            envelope = np.exp(-t * 20)
            hit = np.sin(2 * np.pi * freq * t) * envelope
            end = min(start + hit_samples, len(audio))
            audio[start:end, 0] += hit[:end-start]
            audio[start:end, 1] += hit[:end-start]
        return audio / max(np.max(np.abs(audio)), 0.001) * 0.8
    
    return {
        'kick': create_stem([0.0, 0.5], freq=60),
        'snare': create_stem([0.25, 0.75], freq=200),
        'hihat': create_stem([0.0, 0.25, 0.5, 0.75], freq=8000, decay=0.05),
        'toms': create_stem([], freq=150),  # Empty stem
        'cymbals': create_stem([], freq=5000),  # Empty stem
    }


@pytest.fixture
def test_project(temp_user_files: Path, synthetic_audio: np.ndarray, sample_rate: int) -> Dict[str, Any]:
    """
    Create a complete test project with synthetic audio.
    
    Returns project info dict matching project_manager format.
    """
    project_dir = temp_user_files / "1 - test_song"
    project_dir.mkdir()
    
    # Save synthetic audio
    audio_path = project_dir / "test_song.wav"
    sf.write(str(audio_path), synthetic_audio, sample_rate)
    
    # Create project metadata
    metadata = {
        "name": "test_song",
        "created": "2026-01-18T00:00:00",
        "status": {
            "separated": False,
            "cleaned": False,
            "midi_generated": False,
            "video_rendered": False
        }
    }
    metadata_path = project_dir / ".drumtomidi_project.json"
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f)
    
    # Copy config files
    root_dir = Path(__file__).parent
    shutil.copy(root_dir / "midiconfig.yaml", project_dir / "midiconfig.yaml")
    
    return {
        "number": 1,
        "name": "test_song",
        "path": project_dir,
        "metadata": metadata
    }


@pytest.fixture
def test_project_with_stems(
    test_project: Dict[str, Any],
    synthetic_stems: Dict[str, np.ndarray],
    sample_rate: int
) -> Dict[str, Any]:
    """
    Create a test project with pre-made stems (skips separation).
    """
    project_dir = test_project["path"]
    stems_dir = project_dir / "stems"
    stems_dir.mkdir()
    
    # Save synthetic stems
    for stem_name, audio in synthetic_stems.items():
        stem_path = stems_dir / f"test_song-{stem_name}.wav"
        sf.write(str(stem_path), audio, sample_rate)
    
    # Update metadata
    test_project["metadata"]["status"]["separated"] = True
    metadata_path = project_dir / ".drumtomidi_project.json"
    with open(metadata_path, 'w') as f:
        json.dump(test_project["metadata"], f)
    
    return test_project


# ============================================================================
# Sidechain Cleanup Tests
# ============================================================================

class TestSidechainCleanup:
    """Test sidechain compression cleanup."""
    
    def test_envelope_follower_produces_valid_output(self, sample_rate: int):
        """Smoke test: envelope follower produces valid output."""
        from sidechain_shell import envelope_follower
        
        # Create test audio with a transient
        audio = np.zeros(sample_rate)
        audio[1000:1100] = np.sin(np.linspace(0, 10*np.pi, 100))
        
        envelope = envelope_follower(audio, sample_rate)
        
        assert envelope.shape == audio.shape
        assert np.all(envelope >= 0)
        assert np.max(envelope) > 0  # Should detect the transient
    
    def test_sidechain_compress_reduces_signal(self, sample_rate: int):
        """Property test: sidechain compression reduces main signal when sidechain is loud."""
        from sidechain_shell import sidechain_compress
        
        # Create main audio (constant)
        main = np.ones((sample_rate, 2), dtype=np.float32) * 0.5
        
        # Create sidechain with a loud burst in the middle
        sidechain = np.zeros((sample_rate, 2), dtype=np.float32)
        sidechain[sample_rate//4:sample_rate//2, :] = 0.9
        
        compressed = sidechain_compress(
            main, sidechain, sample_rate,
            threshold_db=-20.0, ratio=10.0
        )
        
        assert compressed.shape == main.shape
        # Signal should be reduced during sidechain burst
        compressed_during_burst = np.mean(np.abs(compressed[sample_rate//4:sample_rate//2]))
        original_level = np.mean(np.abs(main[0:sample_rate//4]))
        assert compressed_during_burst < original_level
    
    def test_cleanup_creates_cleaned_stems(
        self, test_project_with_stems: Dict[str, Any], sample_rate: int
    ):
        """Integration test: cleanup creates cleaned stem files."""
        from sidechain_shell import sidechain_compress
        
        project_dir = test_project_with_stems["path"]
        stems_dir = project_dir / "stems"
        cleaned_dir = project_dir / "cleaned"
        
        # Load stems
        kick_path = stems_dir / "test_song-kick.wav"
        snare_path = stems_dir / "test_song-snare.wav"
        
        kick, sr = sf.read(str(kick_path))
        snare, sr = sf.read(str(snare_path))
        
        # Apply sidechain compression to reduce snare bleed in kick
        cleaned_kick = sidechain_compress(
            kick, snare, sr,
            threshold_db=-30.0,
            ratio=10.0,
            attack_ms=1.0,
            release_ms=100.0
        )
        
        # Save cleaned stem
        cleaned_dir.mkdir(exist_ok=True)
        cleaned_path = cleaned_dir / "test_song-kick.wav"
        sf.write(str(cleaned_path), cleaned_kick, sr)
        
        assert cleaned_path.exists()
        
        # Verify output is valid audio
        loaded, _ = sf.read(str(cleaned_path))
        assert loaded.shape == kick.shape


# ============================================================================
# Stems to MIDI Tests
# ============================================================================

@pytest.fixture
def drum_mapping():
    """Create drum mapping for tests."""
    from stems_to_midi.config import DrumMapping
    return DrumMapping(
        kick=36, snare=38,
        hihat_closed=42, hihat_open=46,
        tom_low=45, tom_mid=47, tom_high=50,
        crash=49, ride=51
    )


class TestStemsToMidi:
    """Test stems to MIDI conversion."""
    
    def test_midi_file_created(self, test_project_with_stems: Dict[str, Any], drum_mapping):
        """Integration test: MIDI file is created from stems."""
        from stems_to_midi.processing_shell import process_stem_to_midi
        from stems_to_midi.midi import create_midi_file
        import yaml
        
        project_dir = test_project_with_stems["path"]
        stems_dir = project_dir / "stems"
        midi_dir = project_dir / "midi"
        midi_dir.mkdir(exist_ok=True)
        
        # Load config
        with open(project_dir / "midiconfig.yaml") as f:
            config = yaml.safe_load(f)
        
        # Extract onset detection parameters
        onset_params = config.get('onset_detection', {})
        onset_threshold = onset_params.get('threshold', 0.3)
        onset_delta = onset_params.get('delta', 0.01)
        onset_wait = onset_params.get('wait', 1)
        hop_length = onset_params.get('hop_length', 512)
        
        # Process kick stem
        kick_path = stems_dir / "test_song-kick.wav"
        notes = process_stem_to_midi(
            kick_path,
            stem_type='kick',
            drum_mapping=drum_mapping,
            config=config,
            onset_threshold=onset_threshold,
            onset_delta=onset_delta,
            onset_wait=onset_wait,
            hop_length=hop_length,
            min_velocity=40,
            max_velocity=127
        )
        
        # Notes should be detected (we put 2 kick hits in synthetic stems)
        assert len(notes) >= 1  # At least some notes detected
        
        # Create MIDI file
        midi_path = midi_dir / "test_song.mid"
        create_midi_file({'kick': notes}, str(midi_path), tempo=120.0)
        
        assert midi_path.exists()
        
        # Verify MIDI file is valid
        import mido
        midi = mido.MidiFile(str(midi_path))
        assert len(midi.tracks) > 0
    
    def test_multiple_stems_combined(self, test_project_with_stems: Dict[str, Any], drum_mapping):
        """Integration test: multiple stems produce combined MIDI."""
        from stems_to_midi.processing_shell import process_stem_to_midi
        from stems_to_midi.midi import create_midi_file
        import yaml
        
        project_dir = test_project_with_stems["path"]
        stems_dir = project_dir / "stems"
        midi_dir = project_dir / "midi"
        midi_dir.mkdir(exist_ok=True)
        
        with open(project_dir / "midiconfig.yaml") as f:
            config = yaml.safe_load(f)
        
        # Extract onset detection parameters
        onset_params = config.get('onset_detection', {})
        onset_threshold = onset_params.get('threshold', 0.3)
        onset_delta = onset_params.get('delta', 0.01)
        onset_wait = onset_params.get('wait', 1)
        hop_length = onset_params.get('hop_length', 512)
        
        events_by_stem = {}
        for stem_type in ['kick', 'snare', 'hihat']:
            stem_path = stems_dir / f"test_song-{stem_type}.wav"
            if stem_path.exists():
                notes = process_stem_to_midi(
                    stem_path,
                    stem_type=stem_type,
                    drum_mapping=drum_mapping,
                    config=config,
                    onset_threshold=onset_threshold,
                    onset_delta=onset_delta,
                    onset_wait=onset_wait,
                    hop_length=hop_length,
                    min_velocity=40,
                    max_velocity=127
                )
                if notes:
                    events_by_stem[stem_type] = notes
        
        # Should have notes from at least one stem
        total_notes = sum(len(v) for v in events_by_stem.values())
        assert total_notes >= 1
        
        # Create combined MIDI
        midi_path = midi_dir / "test_song_combined.mid"
        create_midi_file(events_by_stem, str(midi_path), tempo=120.0)
        
        assert midi_path.exists()


# ============================================================================
# Video Rendering Tests
# ============================================================================

class TestVideoRendering:
    """Test MIDI to video rendering."""
    
    def test_midi_parsing_for_render(self, test_project_with_stems: Dict[str, Any], drum_mapping):
        """Test that MIDI files can be parsed for rendering."""
        from midi_parser import parse_midi_file
        from midi_types import STANDARD_GM_DRUM_MAP
        
        # First create a MIDI file
        from stems_to_midi.processing_shell import process_stem_to_midi
        from stems_to_midi.midi import create_midi_file
        import yaml
        
        project_dir = test_project_with_stems["path"]
        stems_dir = project_dir / "stems"
        midi_dir = project_dir / "midi"
        midi_dir.mkdir(exist_ok=True)
        
        with open(project_dir / "midiconfig.yaml") as f:
            config = yaml.safe_load(f)
        
        # Extract onset detection parameters
        onset_params = config.get('onset_detection', {})
        onset_threshold = onset_params.get('threshold', 0.3)
        onset_delta = onset_params.get('delta', 0.01)
        onset_wait = onset_params.get('wait', 1)
        hop_length = onset_params.get('hop_length', 512)
        
        # Generate MIDI from kick stem
        kick_path = stems_dir / "test_song-kick.wav"
        notes = process_stem_to_midi(
            kick_path,
            stem_type='kick',
            drum_mapping=drum_mapping,
            config=config,
            onset_threshold=onset_threshold,
            onset_delta=onset_delta,
            onset_wait=onset_wait,
            hop_length=hop_length,
            min_velocity=40,
            max_velocity=127
        )
        
        midi_path = midi_dir / "test_song.mid"
        create_midi_file({'kick': notes}, str(midi_path), tempo=120.0)
        
        # Now test parsing for rendering
        drum_notes, duration = parse_midi_file(
            str(midi_path),
            STANDARD_GM_DRUM_MAP
        )
        
        assert duration > 0
        # drum_notes may be empty if notes don't match GM map - that's OK for this test
    
    @pytest.mark.slow
    def test_frame_rendering(self, test_project_with_stems: Dict[str, Any], drum_mapping):
        """Test that video frames can be rendered."""
        from render_midi_video_shell import MidiVideoRenderer
        from stems_to_midi.processing_shell import process_stem_to_midi
        from stems_to_midi.midi import create_midi_file
        import yaml
        
        project_dir = test_project_with_stems["path"]
        stems_dir = project_dir / "stems"
        midi_dir = project_dir / "midi"
        midi_dir.mkdir(exist_ok=True)
        
        # Create MIDI file first
        with open(project_dir / "midiconfig.yaml") as f:
            config = yaml.safe_load(f)
        
        onset_params = config.get('onset_detection', {})
        onset_threshold = onset_params.get('threshold', 0.3)
        onset_delta = onset_params.get('delta', 0.01)
        onset_wait = onset_params.get('wait', 1)
        hop_length = onset_params.get('hop_length', 512)
        
        kick_path = stems_dir / "test_song-kick.wav"
        notes = process_stem_to_midi(
            kick_path,
            stem_type='kick',
            drum_mapping=drum_mapping,
            config=config,
            onset_threshold=onset_threshold,
            onset_delta=onset_delta,
            onset_wait=onset_wait,
            hop_length=hop_length,
            min_velocity=40,
            max_velocity=127
        )
        
        midi_path = midi_dir / "test_song.mid"
        create_midi_file({'kick': notes}, str(midi_path), tempo=120.0)
        
        # Test renderer initialization and MIDI parsing
        renderer = MidiVideoRenderer(width=640, height=360, fps=30)
        notes_for_render, duration = renderer.parse_midi(str(midi_path))
        
        assert duration > 0, "Duration should be positive"
        # Verify renderer has the render method (full rendering tested elsewhere)
        assert hasattr(renderer, 'render'), "Renderer should have render method"


# ============================================================================
# Full Pipeline Integration Test
# ============================================================================

@pytest.mark.slow
class TestFullPipeline:
    """Full end-to-end pipeline tests (slow, use synthetic data)."""
    
    def test_stems_to_midi_to_video_pipeline(self, test_project_with_stems: Dict[str, Any], drum_mapping):
        """
        Test complete pipeline from stems to video (skips separation).
        
        This is the primary integration test to verify refactoring doesn't break
        the main workflow.
        """
        from stems_to_midi.processing_shell import process_stem_to_midi
        from stems_to_midi.midi import create_midi_file
        from render_midi_video_shell import MidiVideoRenderer
        import yaml
        
        project_dir = test_project_with_stems["path"]
        stems_dir = project_dir / "stems"
        midi_dir = project_dir / "midi"
        video_dir = project_dir / "video"
        
        midi_dir.mkdir(exist_ok=True)
        video_dir.mkdir(exist_ok=True)
        
        # Step 1: Convert stems to MIDI
        with open(project_dir / "midiconfig.yaml") as f:
            config = yaml.safe_load(f)
        
        onset_params = config.get('onset_detection', {})
        onset_threshold = onset_params.get('threshold', 0.3)
        onset_delta = onset_params.get('delta', 0.01)
        onset_wait = onset_params.get('wait', 1)
        hop_length = onset_params.get('hop_length', 512)
        
        events_by_stem = {}
        for stem_type in ['kick', 'snare', 'hihat']:
            stem_path = stems_dir / f"test_song-{stem_type}.wav"
            if stem_path.exists():
                notes = process_stem_to_midi(
                    stem_path,
                    stem_type=stem_type,
                    drum_mapping=drum_mapping,
                    config=config,
                    onset_threshold=onset_threshold,
                    onset_delta=onset_delta,
                    onset_wait=onset_wait,
                    hop_length=hop_length,
                    min_velocity=40,
                    max_velocity=127
                )
                if notes:
                    events_by_stem[stem_type] = notes
        
        midi_path = midi_dir / "test_song.mid"
        create_midi_file(events_by_stem, str(midi_path), tempo=120.0)
        
        assert midi_path.exists(), "MIDI file should be created"
        
        # Step 2: Parse MIDI for video rendering
        renderer = MidiVideoRenderer(width=640, height=360, fps=30)
        notes_for_render, duration = renderer.parse_midi(str(midi_path))
        
        assert duration > 0, "Duration should be positive"
        assert len(notes_for_render) > 0, "Should have notes to render"
        
        # Step 3: Verify renderer can be initialized for actual rendering
        # (Full video rendering is too slow for integration tests, 
        # but we verify the pipeline components are connected)
        
        # We don't actually render a full video in tests (too slow),
        # but verify the rendering infrastructure is accessible
        assert hasattr(renderer, 'render'), "Renderer should have render method"
        assert callable(renderer.render), "Render method should be callable"


    def test_cleanup_to_midi_pipeline(self, test_project_with_stems: Dict[str, Any], drum_mapping, sample_rate: int):
        """
        Test pipeline with cleanup step: stems → cleanup → MIDI.
        
        This tests sidechain_shell.py which has 0% coverage.
        """
        from sidechain_shell import sidechain_compress
        from stems_to_midi.processing_shell import process_stem_to_midi
        from stems_to_midi.midi import create_midi_file
        import yaml
        import soundfile as sf
        
        project_dir = test_project_with_stems["path"]
        stems_dir = project_dir / "stems"
        cleaned_dir = project_dir / "cleaned"
        midi_dir = project_dir / "midi"
        
        cleaned_dir.mkdir(exist_ok=True)
        midi_dir.mkdir(exist_ok=True)
        
        # Step 1: Apply sidechain cleanup (mimics sidechain_cleanup.py)
        kick_path = stems_dir / "test_song-kick.wav"
        snare_path = stems_dir / "test_song-snare.wav"
        
        kick, sr = sf.read(str(kick_path))
        snare, sr = sf.read(str(snare_path))
        
        cleaned_kick = sidechain_compress(
            kick, snare, sr,
            threshold_db=-30.0,
            ratio=10.0,
            attack_ms=1.0,
            release_ms=100.0
        )
        
        cleaned_kick_path = cleaned_dir / "test_song-kick.wav"
        sf.write(str(cleaned_kick_path), cleaned_kick, sr)
        
        assert cleaned_kick_path.exists(), "Cleaned kick should be created"
        
        # Step 2: Convert cleaned stem to MIDI
        with open(project_dir / "midiconfig.yaml") as f:
            config = yaml.safe_load(f)
        
        onset_params = config.get('onset_detection', {})
        
        notes = process_stem_to_midi(
            cleaned_kick_path,
            stem_type='kick',
            drum_mapping=drum_mapping,
            config=config,
            onset_threshold=onset_params.get('threshold', 0.3),
            onset_delta=onset_params.get('delta', 0.01),
            onset_wait=onset_params.get('wait', 1),
            hop_length=onset_params.get('hop_length', 512),
            min_velocity=40,
            max_velocity=127
        )
        
        midi_path = midi_dir / "test_song_cleaned.mid"
        create_midi_file({'kick': notes}, str(midi_path), tempo=120.0)
        
        assert midi_path.exists(), "MIDI from cleaned stem should be created"
        
        # Verify MIDI is valid
        import mido
        midi = mido.MidiFile(str(midi_path))
        assert len(midi.tracks) > 0


# ============================================================================
# Separation Tests (Very Slow - Requires ML Model)
# ============================================================================

@pytest.mark.slow
@pytest.mark.skipif(
    not Path("mdx_models/MDX23C-8KFFT-InstVoc_HQ_2.ckpt").exists(),
    reason="MDX23C model not available"
)
class TestSeparation:
    """Test audio separation (requires ML model, very slow)."""
    
    def test_separation_creates_stems(self, test_project: Dict[str, Any]):
        """Integration test: separation creates stem files."""
        from separate import separate_project
        
        project = test_project
        
        # Run separation with minimal settings
        separate_project(
            project,
            model='mdx23c',
            overlap=2,  # Minimum for speed
            device='cpu'  # Use CPU for CI
        )
        
        stems_dir = project["path"] / "stems"
        assert stems_dir.exists()
        
        # Check that stem files were created
        stem_files = list(stems_dir.glob("*.wav"))
        assert len(stem_files) >= 1, "At least one stem should be created"
